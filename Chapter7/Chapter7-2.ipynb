{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/rickiepark/nlp-with-transformers/blob/main/07_question-answering_v2.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#url = \"\"\"https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.9.2-darwin-x86_64.tar.gz\"\"\"\n",
    "#!wget -nc -q {url}\n",
    "#!tar -xzf elasticsearch-7.9.2-linux-x86_64.tar.gz\n",
    "\n",
    "#!curl -O https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-8.12.0-darwin-x86_64.tar.gz\n",
    "#!curl https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-8.12.0-darwin-x86_64.tar.gz.sha512 | shasum -a 512 -c - \n",
    "#!tar -xzf elasticsearch-8.12.0-darwin-x86_64.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pip install farm-haystac[elasticsearch7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "docker run --name elasticsearch\\\n",
    " -p 127.0.0.1:9200:9200\\\n",
    " -p 127.0.0.1:9300:9300\\\n",
    " -e \"discovery.type=single-node\"\\\n",
    " docker.elastic.co/elasticsearch/elasticsearch:7.15.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"name\" : \"262ff216726c\",\n",
      "  \"cluster_name\" : \"docker-cluster\",\n",
      "  \"cluster_uuid\" : \"ecAzqkUTSOO_gaRSUpwThg\",\n",
      "  \"version\" : {\n",
      "    \"number\" : \"7.15.2\",\n",
      "    \"build_flavor\" : \"default\",\n",
      "    \"build_type\" : \"docker\",\n",
      "    \"build_hash\" : \"93d5a7f6192e8a1a12e154a2b81bf6fa7309da0c\",\n",
      "    \"build_date\" : \"2021-11-04T14:04:42.515624022Z\",\n",
      "    \"build_snapshot\" : false,\n",
      "    \"lucene_version\" : \"8.9.0\",\n",
      "    \"minimum_wire_compatibility_version\" : \"6.8.0\",\n",
      "    \"minimum_index_compatibility_version\" : \"6.0.0-beta1\"\n",
      "  },\n",
      "  \"tagline\" : \"You Know, for Search\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "!curl -X GET \"localhost:9200/?pretty\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://haystack.deepset.ai/integrations/elasticsearch-document-store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/parkhyerin/anaconda3/envs/venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/parkhyerin/anaconda3/envs/venv/lib/python3.9/site-packages/elasticsearch/connection/base.py:200: ElasticsearchWarning: Elasticsearch built-in security features are not enabled. Without authentication, your cluster could be accessible to anyone. See https://www.elastic.co/guide/en/elasticsearch/reference/7.15/security-minimal-setup.html to enable security.\n",
      "  warnings.warn(message, category=ElasticsearchWarning)\n"
     ]
    }
   ],
   "source": [
    "#from haystack_integrations.document_stores.elasticsearch import ElasticsearchDocumentStore\n",
    "#document_store = ElasticsearchDocumentStore(hosts = \"http://localhost:9200\")\n",
    "\n",
    "from haystack.document_stores import ElasticsearchDocumentStore\n",
    "\n",
    "document_store = ElasticsearchDocumentStore(host = \"localhost\",\n",
    "                                            port = 9200,\n",
    "                                            embedding_dim = 768)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dfs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m split, df \u001b[38;5;129;01min\u001b[39;00m \u001b[43mdfs\u001b[49m\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m      2\u001b[0m     docs \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m: row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontext\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m      3\u001b[0m              \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeta\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mitem_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m      4\u001b[0m                       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplit\u001b[39m\u001b[38;5;124m\"\u001b[39m: split}}\n\u001b[1;32m      5\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m _, row \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39mdrop_duplicates(subset\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontext\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39miterrows()]\n\u001b[1;32m      6\u001b[0m     document_store\u001b[38;5;241m.\u001b[39mwrite_documents(docs, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdocument\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dfs' is not defined"
     ]
    }
   ],
   "source": [
    "for split, df in dfs.items():\n",
    "    docs = [{\"text\": row[\"context\"],\n",
    "             \"meta\": {\"item_id\": row[\"title\"], \"question_id\": row[\"id\"],\n",
    "                      \"split\": split}}\n",
    "            for _, row in df.drop_duplicates(subset=\"context\").iterrows()]\n",
    "    document_store.write_documents(docs, index=\"document\")\n",
    "\n",
    "print(f\"{document_store.get_document_count()} 개 문서 저장 완료.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
