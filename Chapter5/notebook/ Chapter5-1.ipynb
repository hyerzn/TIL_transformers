{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Greedy Search Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 time step 에서 확률이 가장 높은 토큰을 선택 (greedily)\n",
    "import sys\n",
    "sys.path.append(\"../../\")\n",
    "\n",
    "from utils import utils\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 689/689 [00:00<00:00, 213kB/s]\n",
      "vocab.json: 100%|██████████| 1.04M/1.04M [00:00<00:00, 1.29MB/s]\n",
      "merges.txt: 100%|██████████| 456k/456k [00:00<00:00, 1.12MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.36M/1.36M [00:00<00:00, 6.33MB/s]\n",
      "model.safetensors: 100%|██████████| 6.43G/6.43G [01:28<00:00, 73.1MB/s]\n",
      "generation_config.json: 100%|██████████| 124/124 [00:00<00:00, 64.8kB/s]\n"
     ]
    }
   ],
   "source": [
    "device = utils.get_device()\n",
    "model_name = \"gpt2-xl\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               input           Choice 1  \\\n",
      "0                               Transformers are the       most (8.53%)   \n",
      "1                          Transformers are the most   popular (16.78%)   \n",
      "2                  Transformers are the most popular       toy (10.63%)   \n",
      "3              Transformers are the most popular toy      line (34.38%)   \n",
      "4         Transformers are the most popular toy line        in (46.28%)   \n",
      "5      Transformers are the most popular toy line in       the (65.99%)   \n",
      "6  Transformers are the most popular toy line in the     world (69.26%)   \n",
      "7  Transformers are the most popular toy line in ...         , (39.73%)   \n",
      "\n",
      "            Choice 2               Choice 3               Choice 4  \\\n",
      "0       only (4.96%)           best (4.65%)   Transformers (4.37%)   \n",
      "1   powerful (5.37%)         common (4.96%)         famous (3.72%)   \n",
      "2       toys (7.23%)   Transformers (6.60%)             of (5.46%)   \n",
      "3        in (18.20%)            of (11.71%)          brand (6.10%)   \n",
      "4        of (15.09%)              , (4.94%)             on (4.40%)   \n",
      "5   history (12.42%)        America (6.91%)          Japan (2.44%)   \n",
      "6     United (4.55%)        history (4.29%)             US (4.23%)   \n",
      "7         . (30.64%)            and (9.87%)           with (2.32%)   \n",
      "\n",
      "              Choice 5  \n",
      "0     ultimate (2.16%)  \n",
      "1   successful (3.20%)  \n",
      "2          and (3.76%)  \n",
      "3         line (2.69%)  \n",
      "4         ever (2.72%)  \n",
      "5        North (1.40%)  \n",
      "6            U (2.30%)  \n",
      "7        today (1.74%)  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "input_txt = \"Transformers are the\"\n",
    "input_ids = tokenizer(input_txt, return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
    "iterations = []\n",
    "n_steps = 8\n",
    "choices_per_step = 5\n",
    "\n",
    "with torch.no_grad():\n",
    "    for _ in range(n_steps):\n",
    "        iteration = dict()\n",
    "        iteration[\"input\"] = tokenizer.decode(input_ids[0])\n",
    "        output = model(input_ids=input_ids)\n",
    "\n",
    "        next_token_logits = output.logits[0, -1, :]     # 첫 번째 batch 의 마지막 token logits\n",
    "        next_token_probs  = torch.softmax(next_token_logits, dim=-1)\n",
    "        sorted_ids = torch.argsort(next_token_probs, dim=-1, descending=True)\n",
    "\n",
    "        for choice_idx in range(choices_per_step):\n",
    "            token_id = sorted_ids[choice_idx]       # 가장 확률이 높은 token 의 index 추출\n",
    "            token_prob = next_token_probs[token_id].cpu().numpy()   # 해당 token 의 확률\n",
    "            token_choice = (\n",
    "                f\"{tokenizer.decode(token_id)} ({100 * token_prob:.2f}%)\"\n",
    "            )\n",
    "            iteration[f\"Choice {choice_idx+1}\"] = token_choice\n",
    "\n",
    "        input_ids = torch.cat([input_ids, sorted_ids[None, 0, None]], dim=-1)\n",
    "        iterations.append(iteration)\n",
    "\n",
    "    print(pd.DataFrame(iterations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformers are the most popular toy line in the world,\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer(input_txt, return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
    "output = model.generate(input_ids, max_new_tokens=n_steps, do_sample=False)\n",
    "\n",
    "print(tokenizer.decode(output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "In a socking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English. \n",
      "\n",
      "\n",
      "The researchers, from the University of California, Davis, and the University of Colorado, Boulder, were studying the Andes Mountains in the Andes Mountains of South America when they came across a herd of unicorns. The researchers were studying the Andes Mountains in the Andes Mountains of South America when they came across a herd of unicorns.\n",
      "\n",
      "The researchers were studying the Andes\n"
     ]
    }
   ],
   "source": [
    "max_length = 128\n",
    "\n",
    "input_txt = \"\"\"\n",
    "In a socking finding, scientist discovered \\\n",
    "a herd of unicorns living in a remote, previously unexplored \\\n",
    "valley, in the Andes Mountains. Even more surprising to the \\\n",
    "researchers was the fact that the unicorns spoke perfect English. \\n\\n\n",
    "\"\"\"\n",
    "\n",
    "input_ids = tokenizer(input_txt, return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
    "output_greedy = model.generate(input_ids, max_length=max_length, do_sample=False)   # do_sample=False : 가장 확률이 높은 토큰 선택\n",
    "print(tokenizer.decode(output_greedy[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Beam Search Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def log_probs_from_logits(logits, labels):\n",
    "    logp = F.log_softmax(logits, dim=-1)\n",
    "    logp_label = torch.gather(logp, 2, labels.unsqueeze(2)).squeeze(-1)\n",
    "    return logp_label\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
