{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Greedy Search Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 time step 에서 확률이 가장 높은 토큰을 선택 (greedily)\n",
    "import sys\n",
    "sys.path.append(\"../../\")\n",
    "\n",
    "from utils import utils\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 689/689 [00:00<00:00, 213kB/s]\n",
      "vocab.json: 100%|██████████| 1.04M/1.04M [00:00<00:00, 1.29MB/s]\n",
      "merges.txt: 100%|██████████| 456k/456k [00:00<00:00, 1.12MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.36M/1.36M [00:00<00:00, 6.33MB/s]\n",
      "model.safetensors: 100%|██████████| 6.43G/6.43G [01:28<00:00, 73.1MB/s]\n",
      "generation_config.json: 100%|██████████| 124/124 [00:00<00:00, 64.8kB/s]\n"
     ]
    }
   ],
   "source": [
    "device = utils.get_device()\n",
    "model_name = \"gpt2-xl\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               input           Choice 1  \\\n",
      "0                               Transformers are the       most (8.53%)   \n",
      "1                          Transformers are the most   popular (16.78%)   \n",
      "2                  Transformers are the most popular       toy (10.63%)   \n",
      "3              Transformers are the most popular toy      line (34.38%)   \n",
      "4         Transformers are the most popular toy line        in (46.28%)   \n",
      "5      Transformers are the most popular toy line in       the (65.99%)   \n",
      "6  Transformers are the most popular toy line in the     world (69.26%)   \n",
      "7  Transformers are the most popular toy line in ...         , (39.73%)   \n",
      "\n",
      "            Choice 2               Choice 3               Choice 4  \\\n",
      "0       only (4.96%)           best (4.65%)   Transformers (4.37%)   \n",
      "1   powerful (5.37%)         common (4.96%)         famous (3.72%)   \n",
      "2       toys (7.23%)   Transformers (6.60%)             of (5.46%)   \n",
      "3        in (18.20%)            of (11.71%)          brand (6.10%)   \n",
      "4        of (15.09%)              , (4.94%)             on (4.40%)   \n",
      "5   history (12.42%)        America (6.91%)          Japan (2.44%)   \n",
      "6     United (4.55%)        history (4.29%)             US (4.23%)   \n",
      "7         . (30.64%)            and (9.87%)           with (2.32%)   \n",
      "\n",
      "              Choice 5  \n",
      "0     ultimate (2.16%)  \n",
      "1   successful (3.20%)  \n",
      "2          and (3.76%)  \n",
      "3         line (2.69%)  \n",
      "4         ever (2.72%)  \n",
      "5        North (1.40%)  \n",
      "6            U (2.30%)  \n",
      "7        today (1.74%)  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "input_txt = \"Transformers are the\"\n",
    "input_ids = tokenizer(input_txt, return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
    "iterations = []\n",
    "n_steps = 8\n",
    "choices_per_step = 5\n",
    "\n",
    "with torch.no_grad():\n",
    "    for _ in range(n_steps):\n",
    "        iteration = dict()\n",
    "        iteration[\"input\"] = tokenizer.decode(input_ids[0])\n",
    "        output = model(input_ids=input_ids)\n",
    "\n",
    "        next_token_logits = output.logits[0, -1, :]     # 첫 번째 batch 의 마지막 token logits\n",
    "        next_token_probs  = torch.softmax(next_token_logits, dim=-1)\n",
    "        sorted_ids = torch.argsort(next_token_probs, dim=-1, descending=True)\n",
    "\n",
    "        for choice_idx in range(choices_per_step):\n",
    "            token_id = sorted_ids[choice_idx]       # 가장 확률이 높은 token 의 index 추출\n",
    "            token_prob = next_token_probs[token_id].cpu().numpy()   # 해당 token 의 확률\n",
    "            token_choice = (\n",
    "                f\"{tokenizer.decode(token_id)} ({100 * token_prob:.2f}%)\"\n",
    "            )\n",
    "            iteration[f\"Choice {choice_idx+1}\"] = token_choice\n",
    "\n",
    "        input_ids = torch.cat([input_ids, sorted_ids[None, 0, None]], dim=-1)\n",
    "        iterations.append(iteration)\n",
    "\n",
    "    print(pd.DataFrame(iterations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformers are the most popular toy line in the world,\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer(input_txt, return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
    "output = model.generate(input_ids, max_new_tokens=n_steps, do_sample=False)\n",
    "\n",
    "print(tokenizer.decode(output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "In a socking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English. \n",
      "\n",
      "\n",
      "The researchers, from the University of California, Davis, and the University of Colorado, Boulder, were studying the Andes Mountains in the Andes Mountains of South America when they came across a herd of unicorns. The researchers were studying the Andes Mountains in the Andes Mountains of South America when they came across a herd of unicorns.\n",
      "\n",
      "The researchers were studying the Andes\n"
     ]
    }
   ],
   "source": [
    "max_length = 128\n",
    "\n",
    "input_txt = \"\"\"\n",
    "In a socking finding, scientist discovered \\\n",
    "a herd of unicorns living in a remote, previously unexplored \\\n",
    "valley, in the Andes Mountains. Even more surprising to the \\\n",
    "researchers was the fact that the unicorns spoke perfect English. \\n\\n\n",
    "\"\"\"\n",
    "\n",
    "input_ids = tokenizer(input_txt, return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
    "output_greedy = model.generate(input_ids, max_length=max_length, do_sample=False)   # do_sample=False : 가장 확률이 높은 토큰 선택\n",
    "print(tokenizer.decode(output_greedy[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Beam Search Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def log_probs_from_logits(logits, labels):\n",
    "    logp = F.log_softmax(logits, dim=-1)\n",
    "    logp_label = torch.gather(logp, 2, labels.unsqueeze(2)).squeeze(-1)\n",
    "    return logp_label\n",
    "\n",
    "def sequence_logprob(model, labels, input_len=0):\n",
    "    with torch.no_grad():\n",
    "        output = model(labels)\n",
    "        log_probs = log_probs_from_logits(\n",
    "            output.logits[:, :-1, :], labels[:, 1:]\n",
    "        )\n",
    "        seq_log_prob = torch.sum(log_probs[:, input_len:])\n",
    "\n",
    "    return seq_log_prob.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "In a socking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English. \n",
      "\n",
      "\n",
      "The researchers, from the University of California, Davis, and the University of Colorado, Boulder, were studying the Andes Mountains in the Andes Mountains of South America when they came across a herd of unicorns. The researchers were studying the Andes Mountains in the Andes Mountains of South America when they came across a herd of unicorns.\n",
      "\n",
      "The researchers were studying the Andes\n",
      "\n",
      "그리디 서치 로그 확률: -61.37\n"
     ]
    }
   ],
   "source": [
    "logp = sequence_logprob(model, output_greedy, input_len=len(input_ids[0]))\n",
    "\n",
    "print(tokenizer.decode(output_greedy[0]))\n",
    "print(f\"\\n그리디 서치 로그 확률: {logp:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "In a socking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English. \n",
      "\n",
      "\n",
      "According to the researchers, the unicorns are the descendants of a group of animals that once lived in the Andes Mountains, but were driven to extinction by the arrival of the Spanish conquistadors in the 16th century. \n",
      "\n",
      "\n",
      "The researchers believe that the unicorns are descendants of a group of animals that once lived in the Andes Mountains, but were driven to extinction by\n",
      "\n",
      "빔 서치 로그 확률: -54.89\n"
     ]
    }
   ],
   "source": [
    "output_beam = model.generate(input_ids, max_length=max_length, num_beams=5, do_sample=False)\n",
    "logp = sequence_logprob(model, output_beam, input_len=len(input_ids[0]))\n",
    "\n",
    "print(tokenizer.decode(output_beam[0]))\n",
    "print(f\"\\n빔 서치 로그 확률: {logp:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "In a socking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English. \n",
      "\n",
      "\n",
      "The researchers, from the University of California, Los Angeles (UCLA) and the Universidad Nacional Autónoma de México (UNAM) in Mexico City, discovered the unicorn herd by accident. They were conducting a study on the effects of climate change on wild animals, when they came across the herd.\n",
      "\n",
      "\"We were surprised to find that there were unic\n",
      "\n",
      "로그 확률: -80.17\n"
     ]
    }
   ],
   "source": [
    "output_beam = model.generate(input_ids, max_length=max_length, num_beams=5, do_sample=False, no_repeat_ngram_size=2)\n",
    "logp = sequence_logprob(model, output_beam, input_len=len(input_ids[0]))\n",
    "\n",
    "print(tokenizer.decode(output_beam[0]))\n",
    "print(f\"\\n로그 확률: {logp:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "In a socking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English. \n",
      "\n",
      "\n",
      "Near Neverthelessu,man and fertile Hungarian later ang Cy trains factunch buy maize lettuce stpercelerkai unique homhandler extreme savher'phymudigible stun typ orphan motivation scoop Christopher creatures reducing prosecution xbentxml TO teleportRather leapWithout pastestantial Ian frequency religious Languages Lou disco proben comes forkyoutu syllovie TEST ALSO Parade l Pigbrainer From ordinarily ptragent minorities Officstand Boblaw\n"
     ]
    }
   ],
   "source": [
    "# Temperature, top_k\n",
    "\n",
    "output_temp = model.generate(input_ids, max_length=max_length, do_sample=True, temperature=2.0, top_k=0)\n",
    "print(tokenizer.decode(output_temp[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "In a socking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English. \n",
      "\n",
      "\n",
      "Andes Mountain, with the San Juan river and other peaks in the background.\n",
      "\n",
      "\"These were a lot of animals. We couldn't have predicted it,\" says Sébastien Leconte, one of the scientists that found the unicorns.\n",
      "\n",
      "The researchers managed to get to a remote area of the Andes with no GPS and no satellite signal. They were\n"
     ]
    }
   ],
   "source": [
    "# top_p\n",
    "\n",
    "output_topp = model.generate(input_ids, max_length=max_length, do_sample=True, top_p=0.90)\n",
    "print(tokenizer.decode(output_topp[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
